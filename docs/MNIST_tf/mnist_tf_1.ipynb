{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Blue;\">MNIST handwritten digit classification with TensorFlow<br>\n",
    "    Part 1 - Training and evaluation</h1>\n",
    "<br>\n",
    "<b>\n",
    "This Jupyter NoteBook will explain how to build a simple CNN for classifying the MNIST dataset using the TensorFlow layers API, then how to train and evaluate it. The complete python code (mnist_train.py) can be found in this GitHub repo.\n",
    "<br><br>\n",
    "The CNN we are going to build will look like this:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First, we import the necessary Python packages.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we create some directories for the TensorBoard event logs and the TensorFlow checkpoints. If the directories already exist, we delete them and recreate them so that we are always starting from scratch.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /home/mharvey/ml/mnist_tf/tb_logs created \n",
      "Directory  /home/mharvey/ml/mnist_tf/chkpts created \n"
     ]
    }
   ],
   "source": [
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "TRAIN_GRAPH = 'training_graph.pb'\n",
    "CHKPT_FILE = 'float_model.ckpt'\n",
    "\n",
    "CHKPT_DIR = os.path.join(SCRIPT_DIR, 'chkpts')\n",
    "TB_LOG_DIR = os.path.join(SCRIPT_DIR, 'tb_logs')\n",
    "CHKPT_PATH = os.path.join(CHKPT_DIR, CHKPT_FILE)\n",
    "MNIST_DIR = os.path.join(SCRIPT_DIR, 'mnist_dir')\n",
    "\n",
    "\n",
    "# create a directory for the MNIST dataset if it doesn't already exist\n",
    "if not (os.path.exists(MNIST_DIR)):\n",
    "    os.makedirs(MNIST_DIR)\n",
    "    print(\"Directory \" , MNIST_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "# create a directory for the TensorBoard data if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(TB_LOG_DIR)):\n",
    "    shutil.rmtree(TB_LOG_DIR)\n",
    "os.makedirs(TB_LOG_DIR)\n",
    "print(\"Directory \" , TB_LOG_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "# create a directory for the checkpoint if it doesn't already exist\n",
    "# delete it and recreate if it already exists\n",
    "if (os.path.exists(CHKPT_DIR)):\n",
    "    shutil.rmtree(CHKPT_DIR)\n",
    "os.makedirs(CHKPT_DIR)\n",
    "print(\"Directory \" , CHKPT_DIR ,  \"created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set up the learning rate for the Optimizer, the batch size and the number of epochs. We will only run for 3 epochs to keep the training time to a minimum..be aware that real world machine learning algorithms might need thousands of epochs to train properly.\n",
    "<br>\n",
    "A batch size of 100 and just 3 epochs of training will still gives us between 97% and 98% final accuracy.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 0.0001\n",
    "BATCHSIZE = 100\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">Data Wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Now we download the MNIST dataset. TensorFlow very conveniently has a built-in function to do the job for us. What you get is a dataset that has been split into 60k images and labels for training, 10k images and labels for test. The 'images' are actually numpy arrays with the datatype of each array member set to 8bit unsigned integer. We scale this image data back to the range 0:1.0 by dividing by 255.0. The labels are also integers, so we one-hot encode them using the 'to_categorical()' method.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset has 60k images. Training set is 60k, test set is 10k.\n",
    "# Each image is 28x28x8bits\n",
    "mnist_dataset = tf.keras.datasets.mnist.load_data('mnist_data')\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataset\n",
    "\n",
    "# scale pixel values from 0:255 to 0:1\n",
    "# Also converts uint8 values to float\n",
    "x_train = (x_train/255.0)  \n",
    "x_test = (x_test/255.0)\n",
    "\n",
    "# reshape train & test images\n",
    "x_train = np.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = np.reshape(x_test, [-1, 28, 28, 1])\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "The built-in MNIST download function gives us a split of 60k images for training and 10k images for test. We are going to 'steal' 5k images from the training set to create a validation set of 'unseen' images which we will use to test our trained model.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 5000 images from train set to make a dataset for prediction\n",
    "x_valid = x_train[55000:]\n",
    "y_valid = y_train[55000:]\n",
    "\n",
    "# reduce train dataset to 55000 images\n",
    "y_train = y_train[:55000]\n",
    "x_train = x_train[:55000]\n",
    "\n",
    "# calculate total number of batches\n",
    "total_batches = int(len(x_train)/BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Computational Graph</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "The placeholders for inputting data have shapes that match the modified datasets. The 'x' placeholder takes in the 28pixel x 28pixel images (..actually numpy arrays..) and so has shape [None, 28, 28, 1].  The 'y' placeholder takes in the one-hot encoded labels.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholders for the input data & labels\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name='images_in')\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='labels_in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "Now we define our simple CNN as a series of layers..a 2D convolution layer, followed by a max pooling layer, then another convolution layer, then a flatten layer, then a fully-connected (or dense) layer with 256 outputs before a final fully connected layer with 10 outputs.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN\n",
    "def cnn(x):\n",
    "  net = tf.layers.conv2d(x, 16, [3, 3], activation=tf.nn.relu)\n",
    "  net = tf.layers.max_pooling2d(net, pool_size=2, strides=2)\n",
    "  net = tf.layers.conv2d(net, 32, [3, 3], activation=tf.nn.relu)\n",
    "  net = tf.layers.flatten(net)\n",
    "  net = tf.layers.dense(net, units=256, activation=tf.nn.relu)\n",
    "  logits = tf.layers.dense(net, units=10, activation=None)\n",
    "  return logits\n",
    "\n",
    "# build the network, input comes from the 'x' placeholder\n",
    "logits = cnn(x)\n",
    "prediction = tf.nn.softmax(logits, name='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The loss function is a cross entropy function for classification which accepts labels in one-hot format (..which explains why we one-hot encoded the labels earlier..). The training optimizer is an Adaptive Momentum type.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax cross entropy loss function\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=y))\n",
    "\n",
    "# Adaptive Momentum optimizer - minimize the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARN_RATE, name='Adam').minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will calculate the accuracy of our network as the mean of the correct predictions..</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the prediction matches the label\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\n",
    " # Calculate accuracy as mean of the correct predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will collect the loss and accuracy data for displaying in TensorBoard along with the images that are fed into the 'x' placeholder.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_images:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorBoard data collection\n",
    "tf.summary.scalar('cross_entropy_loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.image('input_images', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We define an instance of a saver object which will be used inside our session to save the trained model checkpoint.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up saver object\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Session</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inside the session, we initialize all the variables then loop through the number of epochs, sending the training data into the 'x' and 'y' placeholders.\n",
    "\n",
    "When we exit the training loop, the final accuracy is calculated and then the final trained model is saved as a checkpoint and as a graph in a protobuf text file.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "TRAINING PHASE\n",
      "-------------------------------------------------------------\n",
      "Epoch 1 / 3\n",
      " Step:    0  Training accuracy: 0.0763\n",
      " Step:  100  Training accuracy: 0.8605\n",
      " Step:  200  Training accuracy: 0.9064\n",
      " Step:  300  Training accuracy: 0.9211\n",
      " Step:  400  Training accuracy: 0.9272\n",
      " Step:  500  Training accuracy: 0.9341\n",
      "Epoch 2 / 3\n",
      " Step:    0  Training accuracy: 0.9388\n",
      " Step:  100  Training accuracy: 0.9425\n",
      " Step:  200  Training accuracy: 0.9499\n",
      " Step:  300  Training accuracy: 0.9500\n",
      " Step:  400  Training accuracy: 0.9539\n",
      " Step:  500  Training accuracy: 0.9599\n",
      "Epoch 3 / 3\n",
      " Step:    0  Training accuracy: 0.9614\n",
      " Step:  100  Training accuracy: 0.9642\n",
      " Step:  200  Training accuracy: 0.9689\n",
      " Step:  300  Training accuracy: 0.9670\n",
      " Step:  400  Training accuracy: 0.9679\n",
      " Step:  500  Training accuracy: 0.9724\n",
      "-------------------------------------------------------------\n",
      "FINISHED TRAINING\n",
      "Run `tensorboard --logdir=/home/mharvey/ml/mnist_tf/tb_logs --port 6006 --host localhost` to see the results.\n",
      "-------------------------------------------------------------\n",
      "EVALUATION PHASE:\n",
      "Final Accuracy with validation set: 0.9788\n",
      "-------------------------------------------------------------\n",
      "SAVING:\n",
      "Saved checkpoint to /home/mharvey/ml/mnist_tf/chkpts/float_model.ckpt\n",
      "Saved binary graphDef to /home/mharvey/ml/mnist_tf/chkpts/training_graph.pb\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = tf.summary.FileWriter(TB_LOG_DIR, sess.graph)\n",
    "    tb_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Training phase with training data\n",
    "    print ('-------------------------------------------------------------')\n",
    "    print ('TRAINING PHASE')\n",
    "    print ('-------------------------------------------------------------')\n",
    "    for epoch in range(EPOCHS):\n",
    "        print (\"Epoch\", epoch+1, \"/\", EPOCHS)\n",
    "\n",
    "        # process all batches\n",
    "        for i in range(total_batches):\n",
    "            \n",
    "            # fetch a batch from training dataset\n",
    "            batch_x, batch_y = x_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE], y_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE]\n",
    "\n",
    "            # Run graph for optimization, loss, accuracy - i.e. do the training\n",
    "            _, s = sess.run([optimizer, tb_summary], feed_dict={x: batch_x, y: batch_y})\n",
    "            writer.add_summary(s, (epoch*total_batches + i))\n",
    "            # Display accuracy per 100 batches\n",
    "            if i % 100 == 0:\n",
    "              acc = sess.run(accuracy, feed_dict={x: x_test, y: y_test})\n",
    "              print (' Step: {:4d}  Training accuracy: {:1.4f}'.format(i,acc))\n",
    "\n",
    "\n",
    "    print ('-------------------------------------------------------------')\n",
    "    print ('FINISHED TRAINING')\n",
    "    print('Run `tensorboard --logdir=%s --port 6006 --host localhost` to see the results.' % TB_LOG_DIR)\n",
    "    print ('-------------------------------------------------------------')\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    # Evaluation phase with validation dataset\n",
    "    print ('EVALUATION PHASE:')\n",
    "    print (\"Final Accuracy with validation set:\", sess.run(accuracy, feed_dict={x: x_valid, y: y_valid}))\n",
    "    print ('-------------------------------------------------------------')\n",
    "\n",
    "    # save post-training checkpoint & graph\n",
    "    print ('SAVING:')\n",
    "    save_path = saver.save(sess, os.path.join(CHKPT_DIR, CHKPT_FILE))\n",
    "    print('Saved checkpoint to %s' % os.path.join(CHKPT_DIR,CHKPT_FILE))\n",
    "    tf.train.write_graph(sess.graph_def, CHKPT_DIR, TRAIN_GRAPH, as_text=False)\n",
    "    print('Saved binary graphDef to %s' % os.path.join(CHKPT_DIR,TRAIN_GRAPH))\n",
    "    print ('-------------------------------------------------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
