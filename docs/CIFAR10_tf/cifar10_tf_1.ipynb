{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:Blue;\">CIFAR-10 image classification with TensorFlow<br>\n",
    "    Part 1 - Training and evaluation</h1>\n",
    "<br>\n",
    "<b>\n",
    "This Jupyter NoteBook will explain how to build a miniVGGNet CNN for classifying the CIFAR-10 dataset using the TensorFlow layers API, then how to train and evaluate it. The complete python code (cifar10_train.py and miniVGGNet.py) can be found in this GitHub repo.\n",
    "<br><br>\n",
    "The MiniVGGNet CNN was developed by Adrian Rosebrock and looks like this:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/minivggnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First, we import the necessary Python packages..</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we create some directories for the TensorBoard event logs and the TensorFlow checkpoints. If the directories already exist, we delete them and recreate them so that we are always starting from scratch.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "INFER_GRAPH = 'inference_graph.pb'\n",
    "CHKPT_FILE = 'float_model.ckpt'\n",
    "\n",
    "CHKPT_DIR = os.path.join(SCRIPT_DIR, 'chkpts')\n",
    "TB_LOG_DIR = os.path.join(SCRIPT_DIR, 'tb_logs')\n",
    "CHKPT_PATH = os.path.join(CHKPT_DIR, CHKPT_FILE)\n",
    "\n",
    "\n",
    "if (os.path.exists(TB_LOG_DIR)):\n",
    "    shutil.rmtree(TB_LOG_DIR)\n",
    "os.makedirs(TB_LOG_DIR)\n",
    "print(\"Directory \" , TB_LOG_DIR ,  \"created \") \n",
    "\n",
    "\n",
    "if (os.path.exists(CHKPT_DIR)):\n",
    "    shutil.rmtree(CHKPT_DIR)\n",
    "os.makedirs(CHKPT_DIR)\n",
    "print(\"Directory \" , CHKPT_DIR ,  \"created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">Data Wrangling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we download the CIFAR-10 dataset. TensorFlow includes the Keras library which has a built-in function to do the job for us. What you get is a dataset that has been split into 50k images and labels for training, 10k images and labels for test. \n",
    "<br>\n",
    "The 'images' are actually numpy arrays with the datatype of each array member set to 8bit unsigned integer. We scale this image data back to the range 0:1.0 by dividing by 255.0. The labels are also integers, so we one-hot encode them using the `to_categorical()` method.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 dataset has 60k images. Training set is 50k, test set is 10k.\n",
    "# Each image is 32x32x8bits\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale image data from range 0:255 to range 0:1\n",
    "# Also converts train & test image data to float from uint8\n",
    "x_train = (x_train/255.0).astype(np.float32)\n",
    "x_test = (x_test/255.0).astype(np.float32)\n",
    "\n",
    "# take 5000 images & labels from the train dataset to create a validation set of 'unseen' images\n",
    "x_valid = x_train[45000:]\n",
    "y_valid = y_train[45000:]\n",
    "\n",
    "# train dataset reduced to 45000 images\n",
    "x_train = x_train[:45000]\n",
    "y_train = y_train[:45000]\n",
    "\n",
    "\n",
    "# one-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set up the learning rate for the Optimizer, the batch size and the number of epochs. We will only run for 3 epochs to keep the training time to a minimum..be aware that real world machine learning algorithms might need humdreds of epochs to train properly.\n",
    "<br>\n",
    "We also calculate the total number of steps per epoch, which is just the size of the training dataset divided by the number of batches.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 0.001\n",
    "EPOCHS = 3\n",
    "BATCHSIZE = 50\n",
    "\n",
    "\n",
    "# calculate total number of batches per epoch\n",
    "total_batches = int(len(x_train)/BATCHSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Computational Graph</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:Blue;\">Define placeholders</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The placeholders are tensors which are used for inputting data when a session is run.__\n",
    "* __The `images_in` placeholder takes in the 32pixel x 32pixel RGB images (..actually numpy arrays..) and so has shape [None,32,32,3].__\n",
    "* __The `labels` placeholder takes in the one-hot encoded labels, so has shape [None,10] and data type integer32.__\n",
    "__The `train` and `drop` placeholders control the action of the batch normalization and dropout layers.__\n",
    "* __`train` is a boolean placeholder which will put the batch norm and droput layers in training mode during training only.__\n",
    "* __`drop` controls the dropout rate of the dropout layers and will be 0.25 during training and 0.0 during validation and testing.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_in = tf.placeholder(tf.float32, shape=[None,32,32,3], name='images_in')\n",
    "labels = tf.placeholder(tf.int32, shape=[None,10], name='labels')\n",
    "train = tf.placeholder_with_default(False, shape=None, name='train')\n",
    "drop = tf.placeholder_with_default(0.0, shape=None, name='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:Blue;\">Define the actual CNN..</h4>\n",
    "<br>\n",
    "<b>\n",
    "The miniVGGNet structure is described in the miniVGGNet.py script in the 'netmodel' folder and looks like this:\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniVGGNet(inputs, is_training, drop_rate):\n",
    "    net = tf.layers.conv2d(inputs=inputs, filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.batch_normalization(inputs=net, training=is_training)\n",
    "    net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.batch_normalization(inputs=net, training=is_training)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
    "    net = tf.layers.dropout(inputs=net, rate=drop_rate, training=is_training)\n",
    "\n",
    "    net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.batch_normalization(inputs=net, training=is_training)\n",
    "    net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "    net = tf.layers.batch_normalization(inputs=net, training=is_training)\n",
    "    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n",
    "    net = tf.layers.dropout(inputs=net, rate=drop_rate, training=is_training)\n",
    "\n",
    "    net = tf.layers.flatten(inputs=net)\n",
    "    net = tf.layers.dense(inputs=net, units=512, activation=tf.nn.relu)\n",
    "    net = tf.layers.batch_normalization(inputs=net, training=is_training)\n",
    "    net = tf.layers.dropout(inputs=net, rate=drop_rate, training=is_training)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=net, units=10, activation=None)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>It is a series of layers, so we use the `tf.layers` API. In our main training script (cifar10_train.py) we just call the miniVGGNet function like this:</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = miniVGGNet(inputs=images_in, is_training=train, drop_rate=drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:Blue;\">Define loss, accuracy and optimizer</h4>\n",
    "\n",
    "<b>The loss function is a cross entropy function for classification which accepts labels in one-hot format (..which explains why we one-hot encoded the labels earlier..). The training optimizer is an Adaptive Momentum type.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax cross entropy loss function\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels))\n",
    "\n",
    "# Adaptive Momentum optimizer - minimize the loss\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARN_RATE, name='Adam')\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will calculate the accuracy of our network during training as the mean of the correct predictions..</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the prediction matches the label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1, output_type=tf.int32), tf.argmax(labels, 1, output_type=tf.int32)  )\n",
    "\n",
    " # Calculate accuracy as mean of the correct predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TensorFlow provides built-in functions for calculating the top-k accuracies..</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top5 = tf.nn.in_top_k(predictions=logits, targets=tf.argmax(labels, 1), k=5)\n",
    "in_top1 = tf.nn.in_top_k(predictions=logits, targets=tf.argmax(labels, 1), k=1)\n",
    "top5_acc = tf.reduce_mean(tf.cast(in_top5, tf.float32))\n",
    "top1_acc = tf.reduce_mean(tf.cast(in_top1, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We will collect the loss and accuracy data for displaying in TensorBoard along with the images that are fed into the 'images_in' placeholder.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard data collection\n",
    "tf.summary.scalar('cross_entropy_loss', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.image('input_images', images_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We define an instance of a saver object which will be used inside our session to save the trained model checkpoint.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up saver object\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">The Session</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inside the session, we initialize all the variables then loop through the number of epochs, sending the training data into the `images_in` and `labels` placeholders.\n",
    "\n",
    "When we exit the training loop, the final top-1 and top-5 accuracy is calculated and then the weights and biases of the trained model are saved as a checkpoint.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "    \n",
    "    # TensorBoard writer\n",
    "    writer = tf.summary.FileWriter(TB_LOG_DIR, sess.graph)\n",
    "    tb_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Training phase with training data\n",
    "    print ('******************************')\n",
    "    print ('TRAINING STARTED..')\n",
    "    print ('******************************\\n')\n",
    "    for epoch in range(EPOCHS):\n",
    "        print (\"Epoch\", epoch+1, \"/\", EPOCHS)\n",
    "\n",
    "        # process all batches\n",
    "        for i in range(total_batches):\n",
    "            \n",
    "            # fetch a batch from training dataset\n",
    "            x_train_batch, y_train_batch = x_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE], y_train[i*BATCHSIZE:i*BATCHSIZE+BATCHSIZE]\n",
    "\n",
    "            # Display training accuracy every 100 batches\n",
    "            if i % 100 == 0:\n",
    "              acc = sess.run(accuracy, feed_dict={images_in: x_test[:1000], labels: y_test[:1000]})\n",
    "              print (' Step: {:4d}  Training accuracy: {:1.4f}'.format(i,acc))\n",
    "\n",
    "            # Run graph for optimization  - i.e. do the training\n",
    "            _, s = sess.run([train_op, tb_summary], feed_dict={images_in: x_train_batch, labels: y_train_batch, train: True, drop: 0.25})\n",
    "            writer.add_summary(s, (epoch*total_batches + i))\n",
    "\n",
    "\n",
    "    print(\"\\nTRAINING FINISHED\\n\")\n",
    "    print ('******************************')\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "    # Validation phase with validation dataset\n",
    "    # calculate top-1 and top-5 accuracy with 'unseen' data\n",
    "    print ('******************************')\n",
    "    print(\"VALIDATION\")\n",
    "    print ('******************************\\n')\n",
    "    t5_acc,t1_acc = sess.run([top5_acc,top1_acc], feed_dict={images_in: x_valid[:1000], labels: y_valid[:1000]})\n",
    "    print (' Top 1 accuracy with validation set: {:1.4f}'.format(t1_acc))\n",
    "    print (' Top 5 accuracy with validation set: {:1.4f}'.format(t5_acc))\n",
    "\n",
    "    # save post-training checkpoint\n",
    "    # this saves all the parameters of the trained network\n",
    "    save_path = saver.save(sess, os.path.join(CHKPT_DIR, CHKPT_FILE))\n",
    "    print('\\nSaved checkpoint to %s' % os.path.join(CHKPT_DIR,CHKPT_FILE))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:Blue;\">Create Inference Graph for use with DNNDK</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The computational graph we have defined above cannot be used with DNNDK. It includes the `train` and `drop` placeholders which feed varying values into the CNN function's `is_training` and `drop_rate` arguments. We need to create a new graph which has the `is_training` and `drop_rate` arguments of our CNN tied to static values: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "  # define placeholders for the input data\n",
    "  x_1 = tf.placeholder(tf.float32, shape=[None,32,32,3], name='images_in')\n",
    "\n",
    "  # call the miniVGGNet function with is_training=False & dropout rate=0\n",
    "  logits_1 = miniVGGNet(x_1, is_training=False, drop_rate=0.0)\n",
    "\n",
    "  tf.train.write_graph(tf.get_default_graph().as_graph_def(), CHKPT_DIR, INFER_GRAPH, as_text=False)\n",
    "  print('Saved binary inference graph to %s' % os.path.join(CHKPT_DIR,INFER_GRAPH))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
